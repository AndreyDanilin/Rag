"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Open RAG Benchmark
"""
import os
import json
import requests
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

class OpenRAGDataSetup:
    """–ö–ª–∞—Å—Å –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Open RAG Benchmark"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.corpus_dir = self.data_dir / "corpus"
        self.corpus_dir.mkdir(parents=True, exist_ok=True)
    
    def download_from_huggingface(self, dataset_name: str = "vectara/open-rag-bench"):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç —Å Hugging Face
        –¢—Ä–µ–±—É–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–∫–∏: pip install datasets
        """
        try:
            from datasets import load_dataset
            
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ {dataset_name} —Å Hugging Face...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
            dataset = load_dataset(dataset_name)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ—Ä–ø—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if 'corpus' in dataset:
                corpus_data = dataset['corpus']
                print(f"üìö –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(corpus_data)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
                
                for i, doc in enumerate(corpus_data):
                    doc_path = self.corpus_dir / f"{doc['id']}.json"
                    with open(doc_path, 'w', encoding='utf-8') as f:
                        json.dump(doc, f, ensure_ascii=False, indent=2)
                    
                    if (i + 1) % 100 == 0:
                        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + 1} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å—ã
            if 'queries' in dataset:
                queries_data = dataset['queries']
                queries_path = self.data_dir / "queries.json"
                with open(queries_path, 'w', encoding='utf-8') as f:
                    json.dump(queries_data, f, ensure_ascii=False, indent=2)
                print(f"üí¨ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(queries_data)} –∑–∞–ø—Ä–æ—Å–æ–≤")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å
            if 'qrels' in dataset:
                qrels_data = dataset['qrels']
                qrels_path = self.data_dir / "qrels.json"
                with open(qrels_path, 'w', encoding='utf-8') as f:
                    json.dump(qrels_data, f, ensure_ascii=False, indent=2)
                print(f"üîó –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(qrels_data)} —Å–≤—è–∑–µ–π –∑–∞–ø—Ä–æ—Å-–¥–æ–∫—É–º–µ–Ω—Ç")
            
            print("‚úÖ –î–∞—Ç–∞—Å–µ—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω!")
            
        except ImportError:
            print("‚ùå –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å Hugging Face —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install datasets")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
    
    def download_sample_papers(self, num_papers: int = 10):
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã —Å—Ç–∞—Ç–µ–π –∏–∑ arXiv –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
        """
        print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ {num_papers} –ø—Ä–∏–º–µ—Ä–æ–≤ —Å—Ç–∞—Ç–µ–π...")
        
        # –ü—Ä–∏–º–µ—Ä—ã ID —Å—Ç–∞—Ç–µ–π –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π arXiv
        sample_paper_ids = [
            "2301.00001",  # cs.AI
            "2301.00002",  # cs.LG
            "2301.00003",  # cs.CV
            "2301.00004",  # cs.CL
            "2301.00005",  # cs.IR
            "2301.00006",  # cs.NE
            "2301.00007",  # cs.RO
            "2301.00008",  # cs.SE
            "2301.00009",  # cs.DC
            "2301.00010",  # cs.DB
        ]
        
        for i, paper_id in enumerate(sample_paper_ids[:num_papers]):
            try:
                # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä —Å—Ç–∞—Ç—å–∏
                sample_paper = self._create_sample_paper(paper_id, i)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º
                paper_path = self.corpus_dir / f"{paper_id}.json"
                with open(paper_path, 'w', encoding='utf-8') as f:
                    json.dump(sample_paper, f, ensure_ascii=False, indent=2)
                
                print(f"   ‚úÖ –°–æ–∑–¥–∞–Ω–∞ —Å—Ç–∞—Ç—å—è {paper_id}")
                
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏ {paper_id}: {e}")
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {num_papers} –ø—Ä–∏–º–µ—Ä–æ–≤ —Å—Ç–∞—Ç–µ–π")
    
    def _create_sample_paper(self, paper_id: str, index: int) -> dict:
        """–°–æ–∑–¥–∞–µ—Ç –ø—Ä–∏–º–µ—Ä —Å—Ç–∞—Ç—å–∏"""
        
        topics = [
            "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏", "–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
            "–æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞", "–∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ",
            "—Ä–æ–±–æ—Ç–æ—Ç–µ—Ö–Ω–∏–∫–∞", "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö"
        ]
        
        categories = [
            ["cs.AI", "cs.LG"], ["cs.LG", "cs.NE"], ["cs.CV", "cs.LG"],
            ["cs.CL", "cs.AI"], ["cs.IR", "cs.LG"], ["cs.RO", "cs.AI"]
        ]
        
        topic = topics[index % len(topics)]
        category = categories[index % len(categories)]
        
        return {
            "id": paper_id,
            "title": f"–ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ {topic}: —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∏ –º–µ—Ç–æ–¥—ã",
            "authors": [f"–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å {i+1}" for i in range(2 + index % 3)],
            "categories": category,
            "abstract": f"–í –¥–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ä–µ—à–µ–Ω–∏—é –∑–∞–¥–∞—á –≤ –æ–±–ª–∞—Å—Ç–∏ {topic}. –†–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç—Å—è —Ä–∞–∑–ª–∏—á–Ω—ã–µ –º–µ—Ç–æ–¥—ã –∏ –∞–ª–≥–æ—Ä–∏—Ç–º—ã, –∏—Ö –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏. –ü—Ä–æ–≤–µ–¥–µ–Ω —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π.",
            "published": "2024-01-01",
            "updated": "2024-01-01",
            "sections": [
                {
                    "text": f"–í–≤–µ–¥–µ–Ω–∏–µ –≤ –æ–±–ª–∞—Å—Ç—å {topic}. –î–∞–Ω–Ω–∞—è –æ–±–ª–∞—Å—Ç—å –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —è–≤–ª—è–µ—Ç—Å—è –æ–¥–Ω–æ–π –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –¥–∏–Ω–∞–º–∏—á–Ω–æ —Ä–∞–∑–≤–∏–≤–∞—é—â–∏—Ö—Å—è –≤ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–µ. –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –∏ –º–µ—Ç–æ–¥–æ–≤ –¥–ª—è —Ä–µ—à–µ–Ω–∏—è —Å–ª–æ–∂–Ω—ã—Ö –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º.",
                    "tables": {},
                    "images": {}
                },
                {
                    "text": f"–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è. –í —Ä–∞–º–∫–∞—Ö –¥–∞–Ω–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±—ã–ª–∏ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã —Å–ª–µ–¥—É—é—â–∏–µ –ø–æ–¥—Ö–æ–¥—ã: —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑, –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –≥–ª—É–±–æ–∫–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏. –ö–∞–∂–¥—ã–π –∏–∑ –º–µ—Ç–æ–¥–æ–≤ –∏–º–µ–µ—Ç —Å–≤–æ–∏ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –æ–±–ª–∞—Å—Ç—å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è.",
                    "tables": {
                        "table_1": "| –ú–µ—Ç–æ–¥ | –¢–æ—á–Ω–æ—Å—Ç—å | –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è |\n|-------|----------|----------------|\n| SVM | 85% | 10 –º–∏–Ω |\n| Random Forest | 88% | 5 –º–∏–Ω |\n| Neural Network | 92% | 30 –º–∏–Ω |"
                    },
                    "images": {}
                },
                {
                    "text": f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–µ. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –≤—ã—Å–æ–∫—É—é —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö –º–µ—Ç–æ–¥–æ–≤ –≤ –æ–±–ª–∞—Å—Ç–∏ {topic}. –î–æ—Å—Ç–∏–≥–Ω—É—Ç–æ —É–ª—É—á—à–µ–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ 15% –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏.",
                    "tables": {},
                    "images": {
                        "figure_1": "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChwGA60e6kgAAAABJRU5ErkJggg=="
                    }
                }
            ]
        }
    
    def setup_data(self, use_huggingface: bool = False, num_samples: int = 10):
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        """
        print("üöÄ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RAG-—Å–∏—Å—Ç–µ–º—ã")
        print("=" * 50)
        
        if use_huggingface:
            self.download_from_huggingface()
        else:
            self.download_sample_papers(num_samples)
        
        print(f"\nüìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {self.data_dir}")
        print("‚úÖ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–∞–Ω–Ω—ã—Ö"""
    import argparse
    
    parser = argparse.ArgumentParser(description="–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è RAG-—Å–∏—Å—Ç–µ–º—ã")
    parser.add_argument("--huggingface", action="store_true", 
                       help="–ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Å Hugging Face")
    parser.add_argument("--samples", type=int, default=10,
                       help="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Å—Ç–∞—Ç–µ–π (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 10)")
    
    args = parser.parse_args()
    
    setup = OpenRAGDataSetup()
    setup.setup_data(
        use_huggingface=args.huggingface,
        num_samples=args.samples
    )

if __name__ == "__main__":
    main()
